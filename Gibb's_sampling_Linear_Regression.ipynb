{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gibb's sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most of the real-world probabilistic models, exact inference of the parameters is intractable. Then sampling algorithms are utilised to obtain some parameter approximations. Such inference algorithms based on numerical sampling are called Markov Chain Monte Carlo (MCMC) techniques.\n",
    ">Sampling from a distribution provides a window onto its properties, which allows us to understand its nature. It allows us to estimate those characteristics – the mean, variance and quantiles – that we usually want as outputs from a Bayesian analysis.<br>\n",
    "> _**A Student's Guide to Bayesian Statistics - Ben Lambert**_ (1)\n",
    "\n",
    "Here, we will look at one such algorithm called Gibb's sampling. \n",
    "> Let $p( x )$ be the target distribution where $x = (x_1 , . . . , x_d )$. Gibbs sampling consists of a random walk on an undirected graph whose vertices correspond to the values of $x = (x_1 , . . . , x_d )$ and in which there is an edge from $x$ to $y$ if $x$ and $y$ differ in only one coordinate. Thus, the underlying graph is like a _d_-dimensional lattice except that the vertices in the same coordinate line form a clique.<br>\n",
    "> _**Foundations of Data Science - Avrim Blum, John Hopcroft, and Ravindran Kannan**_ (2)<br>\n",
    "\n",
    "The calculation of the posterior may not be possible but it is sometimes possible to find the conditional probabilities of the parameters. Gibb's sampling updates each parameter by sampling from its conditional distribution given the other parameters.  \n",
    "\n",
    "Gibb's sampling is more efficient than Metropolis-Hastings algorithm as it accepts all the samples and rejects none.  \n",
    ">If we have to create a map of the underground iron deposits and do not have the resources to look at every single point in the entire desert, then, Gibb's sampling says we start in a random location in the desert, we can use the satellite to determine our next sampling point in the east–west direction. We then move to the new point and use our satellite to determine a random point in the north–south direction and move there. We continue this process, alternating between moving to a random location in the east-west and then north-south directions. This method is the approach used by Gibb's sampling.  \n",
    "> We notice two differences between the moves selected by these samplers. First, when using the Metropolis sampler, we often reject proposed steps and remain at our previous location in a given iteration. This contrasts with the Gibbs sampler, where we always move to a new spot in each iteration. Second, for the Metropolis sampler, we change both our north and east coordinates simultaneously, whereas in the Gibbs case we move along only one direction at a time.  \n",
    "> (1)\n",
    "\n",
    "Gibb's sampling for a model with $2$ variables $\\theta = (\\theta_{1},\\theta_{2})$ and data $x$ consists of the following steps:  \n",
    "Chose a random starting point $(\\theta_{1}^{0},\\theta_{2}^{0})$, then iterate over the following:  \n",
    "1. Chose a parameter update order - $(\\theta_{1},\\theta_{2})$ or $(\\theta_{2},\\theta_{1})$.\n",
    "2. In the order chosen in step $1$, sample from the conditional parameter of the each parameter with the other updated parameters. So, for the chosen order $(\\theta_{1},\\theta_{2})$, first, we sample from $p(\\theta_{1}^{1}|\\theta_{2}^{0},x)$ and then sample from $p(\\theta_{2}^{1}|\\theta_{2}^{1},x)$.  \n",
    "Continue this until convergence.  \n",
    "\n",
    "Let us now try to solve a simple Linear Regression problem with Gibb's sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a normal Linear Regression (LR): $$y_{i} \\sim \\mathcal{N} (\\beta_{0} + \\beta_{1}x_{i},1/\\tau), i = 1, ..., N$$  \n",
    "We are interested in finding the posterior distribution of $\\beta_{0}$ (intercept), $\\beta_{1}$ (slope) and $\\tau$ (inverse of the variance called precision). Assuming $N$ i.i.d observations, the likelihood of this model is,\n",
    "\n",
    "$$\\begin{align}\n",
    "p(y,x|\\beta_{0},\\beta_{1},\\tau) & = L(y_{1}, ..., y_{N}, x_{1}, ..., x_{N}|\\beta_{0},\\beta_{1},\\tau) \\\\\n",
    "& = \\prod_{i=1}^{N} \\mathcal{N}(\\beta_{0} + \\beta_{1}x,1/\\tau) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Let us define conjugate priors of the parameters:  \n",
    "1. $\\beta_{0} \\sim \\mathcal{N} (\\mu_{0},1/\\tau_{0})$\n",
    "2. $\\beta_{1} \\sim \\mathcal{N} (\\mu_{1},1/\\tau_{1})$\n",
    "3. $1/\\tau \\sim Gamma(\\alpha,\\beta)$\n",
    "\n",
    "Now, let us define these in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now derive the updates for $\\beta_{0}$ and $\\beta_{1}$  \n",
    "The general approach is the following:\n",
    "1. Write down the posterior conditional density in log-form\n",
    "2. Discard all the terms that don’t depend on the current sampling variable\n",
    "3. Now, assume this is the density for our sampling variable and all other variables are fixed.\n",
    "4. Find out the distribution of this log-density and that is the conditional sampling density we want.\n",
    "\n",
    "Before we derive the updates, there is one important derivation that we need to be aware of.  \n",
    "Let us assume there is variable $x$ that is normally distributed with mean $\\mu$ and precision $\\tau$,\n",
    "\n",
    "$$\\begin{align}\n",
    "f(x|\\mu,1/\\tau) & = \\sqrt \\frac{\\tau}{2\\pi} \\ e^{-\\frac{\\tau}{2}(x-\\mu)^{2}} \\\\\n",
    "& = \\text{const} \\cdot \\ e^{(-\\frac{\\tau x^{2}}{2} -\\frac{\\tau \\mu^{2}}{2} - \\tau x\\mu)}\\\\\n",
    "ln (f(x|\\mu,1/\\tau)) & = ln(\\text{const}) + ln(e^{(-\\frac{\\tau x^{2}}{2} -\\frac{\\tau \\mu^{2}}{2} - \\tau x\\mu)}) \\\\\n",
    "& = ln(\\text{const}) - \\frac{\\tau x^{2}}{2} - \\frac{\\tau \\mu^{2}}{2} + \\tau x\\mu \\\\\n",
    "& = \\frac{\\tau x^{2}}{2} + \\tau x\\mu \\> \\> \\> \\> \\> \\> \\text{(drop terms that are independent of x)} \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Having derived this, note that,\n",
    "1. The coefficient of $x^{2}$ is $\\frac{-\\tau}{2}$\n",
    "2. The coefficient of $x$ is $\\tau \\mu$\n",
    "\n",
    "### Deriving update for $\\beta_{0}$\n",
    "\n",
    "$$\\begin{align}\n",
    "p(\\beta_{0}|\\beta_{1},\\tau, y, x) & \\propto p(y,x|\\beta_{0},\\beta_{1},\\tau) p(\\beta_{0}) \\\\\n",
    "& \\propto \\prod_{i=1}^{N} \\mathcal{N}(\\beta_0 + \\beta_{1}x_{i},1/\\tau) \\cdot \\mathcal{N}(\\mu_{0},1/\\tau_{0}) \\\\\n",
    "ln(p(\\beta_{0}|\\beta_{1},\\tau, y, x)) & \\propto \\sum_{i=1}^{N} ln(\\mathcal{N}(\\beta_0 + \\beta_{1}x_{i},1/\\tau)) + ln(\\mathcal{N}(\\mu_{0},1/\\tau_{0})) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Expanding the first term on the RHS,\n",
    "$$\\begin{align}\n",
    "\\sum_{i=1}^{N} ln(\\mathcal{N}(\\beta_0 + \\beta_{1}x_{i},1/\\tau)) & = \\sum_{i=1}^{N} ln[\\sqrt \\frac{\\tau}{2\\pi}  e^{-\\frac{\\tau}{2}(y_{i} - (\\beta_0 + \\beta_{1}x_{i}))^2}]  \\\\\n",
    "& = \\sum_{i=1}^{N} (ln[\\sqrt \\frac{\\tau}{2\\pi}] + ln[e^{-\\frac{\\tau y_{i}^{2}}{2} - \\frac{\\tau (\\beta_0 + \\beta_{1}x_{i})^2}{2} + \\tau y_{i} (\\beta_0 + \\beta_{1}x_{i})}]) \\\\\n",
    "& = \\sum_{i=1}^{N} (ln[\\sqrt \\frac{\\tau}{2\\pi}] + [-\\frac{\\tau y_{i}^{2}}{2} - \\frac{\\tau \\beta_0^{2}}{2} - \\frac{\\tau \\beta_{1}^{2} x_{i}^{2}}{2} - \\tau \\beta_{0} \\beta_{1} x_{i} + \\tau y_{i} \\beta_0 + \\tau y_{i} \\beta_{1}x_{i}]) \\\\\n",
    "& \\> \\> \\> \\> \\> \\> \\text{(drop all the terms that do not contain } \\beta_{0}\\text{)} \\\\\n",
    "& = \\sum_{i=1}^{N} (- \\frac{\\tau \\beta_0^{2}}{2} - \\tau \\beta_{0} \\beta_{1} x_{i} + \\tau y_{i} \\beta_0) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Expanding the first term on the RHS,\n",
    "$$\\begin{align}\n",
    "ln(\\mathcal{N}(\\mu_{0},1/\\tau_{0})) & = ln(\\sqrt \\frac{\\tau_{0}}{2\\pi}) +  ln(e^{-\\frac{\\tau_{0}}{2}(\\beta_{0} - \\mu_{0}))^2}) \\\\\n",
    "& = ln(\\sqrt \\frac{\\tau_{0}}{2\\pi}) + (-\\frac{\\tau_{0} \\beta_{0}^{2}}{2} - \\frac{\\tau_{0} \\mu_{0}^{2}}{2} + \\tau_{0} \\beta_{0}\\mu_{0})) \\\\\n",
    "& \\> \\> \\> \\> \\> \\> \\text{(drop all the terms that do not contain } \\beta_{0}\\text{)} \\\\\n",
    "& = -\\frac{\\tau_{0} \\beta_{0}^{2}}{2} + \\tau_{0} \\beta_{0}\\mu_{0} \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Having derived this,\n",
    "1. The coefficient of $\\beta_{0}^{2}$ is $\\frac{-\\tau N}{2} - \\frac{\\tau_{0}}{2}$\n",
    "2. The coefficient of $\\beta_{0}$ is $\\tau \\sum_{i=1}^{N} (y_{i} - \\beta_{1}x_{i}) + \\tau_{0} \\mu_{0}$\n",
    "\n",
    "So, $\\beta_{0}$ looks Gaussian and matching the coefficients of $\\beta_{0}$ with that of variable $x$ derivation from earlier and calculating the mean and precision, we define the conditional density of $\\beta_{0}$ as, $$\\sim \\mathcal{N} (\\frac{\\tau_0 \\mu_0 + \\tau \\sum_{i=1}^{N} (y_{i}- \\beta_{1}x_{i})}{\\tau_0 + \\tau N},\\frac{1}{\\tau_0 + \\tau N})$$\n",
    "\n",
    "Let us implement the update step in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_beta_0(y,x,beta_1,tau,mu_0,tau_0):\n",
    "    N = len(y)\n",
    "    assert len(x) == N\n",
    "    \n",
    "    precision = tau_0 + tau*N\n",
    "    mean      = (tau_0*mu_0 + tau*np.sum(y - beta_1*x)) / precision\n",
    "    \n",
    "    return np.random.normal(mean, 1/np.sqrt(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great progress!  \n",
    "Now, let us find the conditional density of $\\beta_{1}$.\n",
    "\n",
    "### Deriving update for $\\beta_{1}$\n",
    "\n",
    "This derivation is very much similar to that of $\\beta_{0}$. So, go ahead and do it yourself! It is good practice.\n",
    "$$\\begin{align}\n",
    "p(\\beta_{1}|\\beta_{0},\\tau, y, x) & \\propto p(y,x|\\beta_{0},\\beta_{1},\\tau) p(\\beta_{1}) \\\\\n",
    "& \\propto  \\frac{-\\tau_{1} \\beta_{1}^2}{2} -\\frac{\\tau \\sum_{i=1}^{N} \\beta_{1}^{2} x_{i}^{2}}{2} + \\tau_{1} \\beta_{1} \\mu_{1} + \\tau \\beta_{1} \\sum_{i=1}^{N} x_{i}(y_{i} - \\beta_{0}) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Having derived this,\n",
    "1. The coefficient of $\\beta_{1}^{2}$ is $\\frac{-\\tau_{1}}{2} - \\frac{\\tau \\sum_{i=1}^{N} x_{i}^{2}}{2}$\n",
    "2. The coefficient of $\\beta_{1}$ is $\\tau \\sum_{i=1}^{N} x_{i}(y_{i} - \\beta_{0}) + \\tau_{1} \\mu_{1}$\n",
    "\n",
    "So, $\\beta_{1}$ looks Gaussian and matching the coefficients of $\\beta_{1}$ with that of variable $x$ derivation from earlier and calculating the mean and precision, we define the conditional density of $\\beta_{1}$ as, $$\\sim \\mathcal{N} (\\frac{\\tau_{1} \\mu_{1} + \\tau \\sum_{i=1}^{N} x_{i}(y_{i}- \\beta_{0})}{\\tau_{1} + \\tau \\sum_{i=1}^{N}x_{i}^{2}},\\frac{1}{\\tau_{1} + \\tau \\sum_{i=1}^{N}x_{i}^{2}})$$\n",
    "\n",
    "Let us implement the update step in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_beta_1(y,x,beta_0,tau,mu_1,tau_1):\n",
    "    N = len(y)\n",
    "    assert len(x) == N\n",
    "    \n",
    "    precision = tau_1 + tau*np.sum(x*x)\n",
    "    mean      = (tau_1*mu_1 + tau*np.sum(x*(y - beta_0))) / precision\n",
    "    \n",
    "    return np.random.normal(mean, 1/np.sqrt(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, onto one last derivation.  \n",
    "### Deriving update for $\\tau$\n",
    "\n",
    "Since this is non-gaussian, let us again derive a Gamma function with variable $x$.\n",
    "$$\\begin{align}\n",
    "p(x|\\alpha,\\beta) & = \\frac{1}{\\Gamma} \\beta^{\\alpha} x^{\\alpha-1} e^{-\\beta x} \\\\\n",
    "ln(p(x|\\alpha,\\beta))& = ln(\\frac{1}{\\Gamma(\\alpha)}) + \\alpha ln(\\beta) + (\\alpha-1)ln(x) - \\beta x \\\\\n",
    "& \\> \\> \\> \\> \\> \\> \\text{(drop all the terms that do not contain } x \\text{)} \\\\\n",
    "& \\propto (\\alpha - 1) ln(x) - \\beta x \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Having derived this,\n",
    "1. The coefficient of $\\tau$ is $-\\beta$\n",
    "2. The coefficient of $ln(\\tau)$ is $\\alpha - 1$\n",
    "\n",
    "Now, we want,\n",
    "$$\\begin{align}\n",
    "p(\\tau|\\beta_{0},\\beta_{1}, y, x) & \\propto p(y,x|\\beta_{0},\\beta_{1},\\tau) p(\\tau) \\\\\n",
    "& \\propto \\prod_{i=1}^{N} \\mathcal{N}(y_{i}|\\beta_0 + \\beta_{1}x_{i},1/\\tau) \\cdot p(x|\\alpha,\\beta) \\\\\n",
    "ln(p(\\tau|\\beta_{0},\\beta_{1}, y, x)) & \\propto \\sum_{i=1}^{N} ln[\\sqrt \\frac{\\tau}{2\\pi} e^{\\frac{-\\tau}{2}(y_{i} - (\\beta_0 + \\beta_{1}x_{i}))^2}] + ln[\\frac{1}{\\Gamma(\\alpha)} \\beta^{\\alpha} \\tau^{\\alpha-1} e^{-\\beta \\tau}] \\\\\n",
    "& \\> \\> \\> \\> \\> \\> \\text{(drop all the terms that do not contain } \\tau \\text{)} \\\\ \n",
    "& \\propto \\sum_{i=1}^{N}(\\frac{1}{2}ln(\\tau) - \\frac{\\tau}{2}(y_{i} - (\\beta_{0}+\\beta_{1}x_{i}))^2) + (\\alpha-1)ln(\\tau) - \\beta \\tau \\\\\n",
    "& \\propto \\frac{N}{2}ln(\\tau) - \\frac{\\tau}{2}\\sum_{i=1}^{N}(y_{i} - \\beta_{0} - \\beta_{1}x_{i})^2 + (\\alpha-1)ln(\\tau) - \\beta \\tau \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Making the coefficient like earlier, we get,\n",
    "1. The coefficient of $\\tau$ is $- \\frac{\\tau}{2}\\sum_{i=1}^{N}(y_{i} - \\beta_{0} - \\beta_{1}x_{i})^2 - \\beta$\n",
    "2. The coefficient of $ln(\\tau)$ is $\\frac{N}{2} + \\alpha - 1$\n",
    "\n",
    "The conditional density of the $\\tau$ update step is,\n",
    "$$\\sim Ga (\\alpha+\\frac{N}{2},\\beta+\\frac{1}{2}\\sum_{i=1}^{N}(y_{i} - \\beta_{0} - \\beta_{1}x_{i})^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_tau(y,x,beta_0,beta_1,alpha,beta):\n",
    "    N = len(y)\n",
    "    assert len(x) == N\n",
    "    \n",
    "    alpha_new = alpha + N/2\n",
    "    beta_new  = beta + np.sum((y - beta_0 - beta_1*x)**2)/2\n",
    "    \n",
    "    return np.random.gamma(alpha_new,1/beta_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the derivations are done!\n",
    "\n",
    "### Setup a toy example\n",
    "\n",
    "Let us create a toy example to test the functions. Let  \n",
    "$\\beta_{0} = -2$,  \n",
    "$\\beta_{1} = 5$,  \n",
    "$\\tau = 3$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0_true = -2\n",
    "beta_1_true = 5\n",
    "tau_true    = 3\n",
    "\n",
    "N = 50\n",
    "x = np.random.uniform(0,4,N)\n",
    "y = np.random.normal(beta_0_true+beta_1_true*x,1/np.sqrt(tau_true))\n",
    "\n",
    "plt.figure()\n",
    "synth_plot = plt.plot(x,y,\"o\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onto defining the Gibb's sampler itself\n",
    "\n",
    "Let us first specify a random initial values for the parameters and define specific hyperparameters, like, $\\beta_{0}$ and $\\beta_{1}$ can be $\\mathcal{N}(0,1)$ and $\\tau$ can be $Gamma(2,1)$.  \n",
    "We can then define the Gibb's sampler according to the update steps mentioned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify initial values\n",
    "init = {\"beta_0\": 0,\n",
    "        \"beta_1\": 0,\n",
    "        \"tau\": 2}\n",
    "\n",
    "# Specify hyper parameters\n",
    "hypers = {\"mu_0\": 0,\n",
    "         \"tau_0\": 1,\n",
    "         \"mu_1\": 0,\n",
    "         \"tau_1\": 1,\n",
    "         \"alpha\": 2,\n",
    "         \"beta\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs(y, x, iters, init, hypers):\n",
    "    assert len(y) == len(x)\n",
    "    beta_0 = init[\"beta_0\"]\n",
    "    beta_1 = init[\"beta_1\"]\n",
    "    tau = init[\"tau\"]\n",
    "    \n",
    "    store_param = np.zeros((iters, 3))\n",
    "    \n",
    "    for i in range(iters):\n",
    "        \n",
    "        # Update beta_0 with other old parameters\n",
    "        beta_0 = sample_beta_0(y, x, beta_1, tau, hypers[\"mu_0\"], hypers[\"tau_0\"])\n",
    "        \n",
    "        # Update beta_1 with new beta_0 and old tau\n",
    "        beta_1 = sample_beta_1(y, x, beta_0, tau, hypers[\"mu_1\"], hypers[\"tau_1\"])\n",
    "        \n",
    "        # Finally update tau with new beta_0 and new beta_1\n",
    "        tau    = sample_tau(y, x, beta_0, beta_1, hypers[\"alpha\"], hypers[\"beta\"])\n",
    "        \n",
    "        # Store the parameters sampled at every iteration\n",
    "        store_param[i,:] = np.array((beta_0, beta_1, tau))\n",
    "        \n",
    "    store_param = pd.DataFrame(store_param)\n",
    "    store_param.columns = ['beta_0', 'beta_1', 'tau']\n",
    "        \n",
    "    return store_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 500\n",
    "\n",
    "# Run the gibb's sampler for 500 iterations and get the trace of the parameters sampled for plotting in the next cell\n",
    "store_param = gibbs(y, x, iters, init, hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_plot = store_param.plot()\n",
    "param_plot.set_xlabel(\"Iteration\")\n",
    "param_plot.set_ylabel(\"Parameter value\")\n",
    "param_plot.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look great! **Congratulations!**  \n",
    "\n",
    "The true values for the toy model were $\\beta_{0} = -2$, $\\beta_{1} = 5$, $\\tau = 3$. From the plot above we observe that after 20-30 iterations the sampler is already close to true result. The number of iterations needed for the sampler's exploration to take place and to start converging to the true posterior parameters is called the burn-in period. Let us look at the statistics of the parameters after this burn-in period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats and histogram of all parameters after 300 iterations (or burn-in)\n",
    "\n",
    "print(\"Parameter means\")\n",
    "print(store_param[:-200].median())\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Parameter standard deviations\")\n",
    "print(store_param[:-200].std())\n",
    "\n",
    "print()\n",
    "\n",
    "hist_plot = store_param[:-200].hist(bins = 30, layout = (1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now predicted the posterior parameter values to under a standard deviation error.  \n",
    "\n",
    "So, here we have successfully finished Bayesian Linear Regression using the Gibb's sampling approach.  \n",
    "\n",
    "I also have more on Gibb's sampling - **Inferring Gaussian Mixture Models (GMM) parameters using Gibb's sampling**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
